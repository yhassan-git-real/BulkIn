# âš¡ BulkIn - Quick Start Guide

Get up and running with BulkIn in **5 minutes**!

---

## ï¿½ Prerequisites

```bash
âœ“ .NET 8 SDK
âœ“ SQL Server 2019+
âœ“ Windows 10/11
```

---

## ï¿½ğŸš€ Step-by-Step Setup

### 1ï¸âƒ£ Database Setup

Run the database script:

```bash
sqlcmd -S YOUR_SERVER\INSTANCE -i scripts\DatabaseSetup.sql
```

Or manually execute `scripts/DatabaseSetup.sql` in SQL Server Management Studio.

**This creates:**
- Database: `RAW_PROCESS`
- Table: `TextFileData` (ID, Data, Filename, Date)
- Temp Table: `TempTextFileData`
- Stored Procedure: `usp_TransferDataFromTemp`

---

### 2ï¸âƒ£ Configure Application

Edit `src/BulkIn/appsettings.json`:

```json
{
  "DatabaseSettings": {
    "ServerName": "YOUR_SERVER\\INSTANCE",     // â† Change this
    "DatabaseName": "RAW_PROCESS",
    "UseTrustedConnection": true
  },
  "FileSettings": {
    "SourceFilePath": "D:\\YourPath\\SourceFiles",  // â† Change this
    "FilePatterns": [ "*.txt", "*.csv" ]
  }
}
```

---

### 3ï¸âƒ£ Prepare Your Files

Place your text files in the source directory:

```
D:\YourPath\SourceFiles\
â”œâ”€â”€ file1.txt
â”œâ”€â”€ file2.csv
â””â”€â”€ data.log
```

---

### 4ï¸âƒ£ Run the Application

**Option A: Quick Run**
```bash
cd src\BulkIn
dotnet run
```

**Option B: Use Batch Files**
```bash
RunBulkIn.bat         # Auto-compile and run
BulkIn-Menu.bat       # Interactive menu
RunBulkIn-Fast.bat    # Direct executable
```

---

## ğŸ“Š What Happens Next?

```
1. Configuration loads âœ…
2. Database connection tests âœ…
3. Files discovered (e.g., 18 files found)
4. Press ENTER to start processing
5. Progress shown in real-time
6. Final summary displayed
```

---

## ğŸ¯ Expected Output

```
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“„ [1/18] yourfile.txt
   354.18 MB â€¢ 2025-10-31 01:36:45
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   ğŸ”§ Preparing temp table... âœ…
   ğŸ“Š Processing: 200,000 rows...
   ğŸ“¥ Inserted: 923,843 rows
   ğŸ”„ Transferring to target... âœ… (923,843 rows)
   âœ… Completed: 923,843 rows â€¢ 23.7s â€¢ 38,983 rows/sec
```

---

## ğŸ“ Check Results

Query your data:

```sql
USE RAW_PROCESS;

-- View all data
SELECT * FROM TextFileData;

-- Count rows per file
SELECT Filename, COUNT(*) AS RowCount
FROM TextFileData
GROUP BY Filename
ORDER BY Filename;

-- View recent inserts
SELECT TOP 100 * 
FROM TextFileData
ORDER BY Date DESC;
```

---

## ğŸ”§ Common Issues

| Issue | Fix |
|-------|-----|
| **Connection failed** | Update `ServerName` in appsettings.json |
| **Files not found** | Check `SourceFilePath` exists |
| **Permission denied** | Run as Administrator |

---

## ğŸ“ Logs

Check logs for detailed information:

```
logs/
â”œâ”€â”€ SuccessLog_YYYYMMDD_HHMMSS.txt
â””â”€â”€ ErrorLog_YYYYMMDD_HHMMSS.txt
```

---

## ğŸ‰ Success!

You're now processing bulk text files at **35,000-65,000 rows/second**!

For advanced configuration, see the main [README.md](README.md).

---

**Questions?** Open an issue on GitHub!

**Version:** 1.0  
**Date:** October 31, 2025  
**Status:** Production-Ready MVP

---

## âš¡ 5-Minute Setup

### **Prerequisites**
âœ… Windows 10/11  
âœ… .NET 8 SDK ([Download](https://dotnet.microsoft.com/download/dotnet/8.0))  
âœ… SQL Server 2019+ (Express/Standard/Enterprise)  
âœ… SQL Server Management Studio (SSMS) - Optional but recommended  

---

## ğŸ“‹ Setup Steps

### **Step 1: Database Setup** (2 minutes)

1. Open SQL Server Management Studio (SSMS)
2. Connect to your SQL Server instance
3. Open the file: `BulkIn\scripts\DatabaseSetup.sql`
4. Execute the script (F5)

**Expected Result:**
```
Database TextFileDatabase created successfully.
Table TextFileData created successfully.
Table TempTextFileData created successfully.
```

---

### **Step 2: Configure Application** (1 minute)

1. Navigate to: `BulkIn\src\BulkIn\`
2. Open `appsettings.json` in any text editor
3. Update the following settings:

```json
{
  "DatabaseSettings": {
    "ServerName": "localhost\\SQLEXPRESS",     â† Change to your SQL Server
    "DatabaseName": "TextFileDatabase"
  },
  "FileSettings": {
    "SourceFilePath": "D:\\SourceFiles",      â† Change to your file folder
    "FilePattern": "*.txt"
  },
  "LoggingSettings": {
    "LogFilePath": "D:\\Project_TextFile\\BulkIn\\logs"  â† Change to your log folder
  }
}
```

**Important:**
- Use double backslashes `\\` in paths (JSON format)
- Ensure the source file path exists
- Log path will be created automatically

---

### **Step 3: Prepare Your Files** (1 minute)

1. Create your source folder (e.g., `D:\SourceFiles`)
2. Place your text files there (e.g., `ACEFOI20250804.txt`)
3. Ensure files are not locked by another application

**Supported Formats:**
- âœ… Plain text files (.txt)
- âœ… CSV files (.csv) - treated as text
- âœ… UTF-8 encoding
- âœ… Any file size (tested up to 500 MB)
- âœ… Any content (whitespace preserved)

---

### **Step 4: Build Application** (30 seconds)

Open PowerShell in the `BulkIn` folder and run:

```powershell
dotnet build src\BulkIn\BulkIn.csproj
```

**Expected Output:**
```
Build succeeded.
    0 Warning(s)
    0 Error(s)
```

---

### **Step 5: Run Application** (30 seconds)

```powershell
dotnet run --project src\BulkIn\BulkIn.csproj
```

**You'll see:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    BulkIn v1.0                             â•‘
â•‘         Bulk Text File Data Ingestion System               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âš™ï¸  Loading configuration...
âœ“ Configuration loaded successfully
...
```

---

## ğŸ¯ Usage Example

### **Scenario:** Process 3 text files into SQL Server

**Files:**
- `DataFile1.txt` (10 MB, 100,000 lines)
- `DataFile2.txt` (25 MB, 250,000 lines)
- `DataFile3.txt` (50 MB, 500,000 lines)

**Steps:**
1. Place files in `D:\SourceFiles`
2. Run BulkIn application
3. Press ENTER when prompted
4. Wait for processing to complete

**Expected Output:**
```
ğŸ“ Discovering files...
âœ“ Found 3 file(s) to process

Files to be processed:
   1. DataFile1.txt
   2. DataFile2.txt
   3. DataFile3.txt

Press ENTER to start processing (or Ctrl+C to cancel):

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              STARTING BATCH PROCESSING                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[1/3] Processing: DataFile1.txt
   â”œâ”€ Reading and inserting: DataFile1.txt
   â”œâ”€ Bulk insert complete: 100,000 rows
   â”œâ”€ Transfer complete: 100,000 rows
âœ“ DataFile1.txt - 100,000 rows in 5.2s

[2/3] Processing: DataFile2.txt
   â”œâ”€ Reading and inserting: DataFile2.txt
   â”œâ”€ Bulk insert complete: 250,000 rows
   â”œâ”€ Transfer complete: 250,000 rows
âœ“ DataFile2.txt - 250,000 rows in 12.8s

[3/3] Processing: DataFile3.txt
   â”œâ”€ Reading and inserting: DataFile3.txt
   â”œâ”€ Bulk insert complete: 500,000 rows
   â”œâ”€ Transfer complete: 500,000 rows
âœ“ DataFile3.txt - 500,000 rows in 25.5s

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘            BULK PROCESSING COMPLETE - SUMMARY             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š OVERALL STATISTICS:
   Total Files Discovered: 3
   âœ“ Successfully Processed: 3
   âœ— Failed: 0
   Success Rate: 100.00%

ğŸ“ˆ DATA METRICS:
   Total Rows Processed: 850,000
   Average Speed: 19,535 rows/second

â±ï¸  TIMING:
   Total Duration: 00:00:43
```

---

## ğŸ“Š Verify Results in SQL Server

### **Option 1: SSMS Query**
```sql
USE TextFileDatabase;
GO

-- Check total rows
SELECT COUNT(*) AS TotalRows FROM TextFileData;

-- Check rows by file
SELECT 
    Filename,
    COUNT(*) AS RowCount,
    MIN(Date) AS FirstInserted,
    MAX(Date) AS LastInserted
FROM TextFileData
GROUP BY Filename
ORDER BY Filename;

-- View sample data
SELECT TOP 10 
    ID,
    LEFT(Data, 100) AS DataPreview,
    Filename,
    Date
FROM TextFileData
ORDER BY ID;
```

### **Expected Results:**
```
TotalRows: 850,000

Filename         RowCount   FirstInserted        LastInserted
---------------------------------------------------------------------------
DataFile1.txt    100,000    2025-10-31 14:30:00  2025-10-31 14:30:00
DataFile2.txt    250,000    2025-10-31 14:30:05  2025-10-31 14:30:05
DataFile3.txt    500,000    2025-10-31 14:30:18  2025-10-31 14:30:18
```

---

## ğŸ“ Check Log Files

Logs are automatically created in your configured log path:

**Success Log:** `logs\SuccessLog_20251031_143000.txt`
```
[2025-10-31 14:30:00] Session Started
[2025-10-31 14:30:05] File: DataFile1.txt - SUCCESS - 100,000 rows
[2025-10-31 14:30:18] File: DataFile2.txt - SUCCESS - 250,000 rows
[2025-10-31 14:30:43] File: DataFile3.txt - SUCCESS - 500,000 rows
[2025-10-31 14:30:43] Batch Complete - 3/3 files successful
```

**Error Log:** `logs\ErrorLog_20251031_143000.txt`
- Empty if no errors occurred
- Contains detailed error messages and stack traces if errors occur

---

## âš™ï¸ Configuration Options

### **Key Settings You Can Adjust:**

| Setting | Purpose | Default | Range |
|---------|---------|---------|-------|
| `BatchSize` | Rows per batch | 50,000 | 10,000 - 100,000 |
| `CommandTimeout` | SQL timeout (seconds) | 600 | 60 - 3600 |
| `ContinueOnError` | Continue if file fails | true | true/false |
| `StreamReaderBufferSize` | Read buffer (bytes) | 65,536 | 1,024 - 1,048,576 |
| `FilePattern` | File matching pattern | *.txt | *.txt, *.csv, *.* |
| `ProcessInAlphabeticalOrder` | Sort files A-Z | true | true/false |

### **Example: Process Larger Batches**
```json
"ProcessingSettings": {
  "BatchSize": 100000,        â† Double the batch size
  "CommandTimeout": 900       â† Increase timeout to 15 minutes
}
```

---

## ğŸ”§ Troubleshooting

### **Problem: "Configuration file not found"**
**Solution:**
- Ensure `appsettings.json` is in `src\BulkIn\` folder
- Check the file is not renamed or moved

### **Problem: "Database connection failed"**
**Solution:**
- Verify SQL Server is running
- Check `ServerName` in appsettings.json
- Test connection in SSMS first
- For SQL Authentication, add Username/Password:
  ```json
  "UseTrustedConnection": false,
  "Username": "sa",
  "Password": "YourPassword"
  ```

### **Problem: "Table does not exist"**
**Solution:**
- Run `DatabaseSetup.sql` script
- Verify database name matches configuration
- Check table names: `TextFileData` and `TempTextFileData`

### **Problem: "Access denied to file"**
**Solution:**
- Close the file in Excel, Notepad, etc.
- Check file is not read-only
- Verify user has read permissions

### **Problem: "Out of memory"**
**Solution:**
- Reduce `BatchSize` to 25,000 or lower
- Close other applications
- Process files individually (not 30 at once)

### **Problem: "Transaction log full"**
**Solution:**
```sql
-- Switch to SIMPLE recovery model
USE master;
ALTER DATABASE TextFileDatabase SET RECOVERY SIMPLE;

-- Shrink transaction log
USE TextFileDatabase;
DBCC SHRINKFILE (TextFileDatabase_log, 1);
```

---

## ğŸ“ Tips & Best Practices

### **For Best Performance:**
âœ… Use `SIMPLE` recovery model during bulk loads  
âœ… Process during off-peak hours  
âœ… Disable indexes on target table during bulk load  
âœ… Rebuild indexes after processing  
âœ… Use local SQL Server (not remote)  
âœ… Close unnecessary applications  

### **For Data Integrity:**
âœ… Always run a test with 1-2 files first  
âœ… Verify row counts after processing  
âœ… Keep backup of original files  
âœ… Check log files for any warnings  
âœ… Test whitespace preservation with sample data  

### **For Large Batches (20+ files):**
âœ… Set `ContinueOnError: true`  
âœ… Monitor log files during processing  
âœ… Ensure adequate disk space (3x file size)  
âœ… Consider processing in smaller groups  
âœ… Schedule during maintenance windows  

---

## ğŸ“ Need Help?

### **Check These First:**
1. âœ… Log files in `logs\` folder
2. âœ… Console output messages
3. âœ… SQL Server error logs
4. âœ… File permissions

### **Common Questions:**

**Q: Can I process CSV files?**  
A: Yes! Use `"FilePattern": "*.csv"` in configuration.

**Q: Will it trim my data?**  
A: NO! All whitespace is preserved exactly as in the file.

**Q: Can I process files in parallel?**  
A: Not in v1.0. Files are processed sequentially for data integrity.

**Q: What if processing fails mid-batch?**  
A: Each file is in its own transaction. Failed files rollback automatically. Successful files remain committed.

**Q: Can I resume a failed batch?**  
A: Move successfully processed files to a different folder, then re-run.

---

## ğŸ¯ What's Next?

After successful processing:

1. âœ… **Verify Data:** Query SQL Server to confirm row counts
2. âœ… **Check Logs:** Review success/error logs
3. âœ… **Backup Data:** Take a database backup
4. âœ… **Archive Files:** Move processed files to archive folder
5. âœ… **Schedule:** Set up automated runs if needed

---

## ğŸ“š Additional Resources

- **README.md** - Complete project documentation
- **DatabaseSetup.sql** - Database schema details
- **MVP_COMPLETE_SUMMARY.md** - Full feature list
- **BulkIn_Roadmap_TrackingPlan.md** - Development details

---

## âœ¨ Success Checklist

Before starting production use:

- [ ] SQL Server database created
- [ ] Tables exist (TextFileData, TempTextFileData)
- [ ] appsettings.json configured correctly
- [ ] Source file path exists
- [ ] Test run completed successfully
- [ ] Row counts verified in SQL Server
- [ ] Log files reviewed
- [ ] Backup strategy in place

---

**You're ready to go! Happy bulk loading! ğŸš€**

---

**Document Version:** 1.0  
**Last Updated:** October 31, 2025  
**Status:** Production-Ready
